<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>大模型推理 - GPTQ 落地与优化 - 书写|记下人生痕迹</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="bugliu" /><meta name="description" content="回顾前一篇文章，GPTQ给大模型带来了降本的可能，但存在性能不佳的问题，无法直接落地。经过迁移适配，我们将GPTQ的INT4 Kernel集成" /><meta name="keywords" content="阅读, 思考, 书写, 程序员, C&#43;&#43;" />






<meta name="generator" content="Hugo 0.109.0 with theme even" />


<link rel="canonical" href="http://towriting.com/blog/2023/06/19/gptq2/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="大模型推理 - GPTQ 落地与优化" />
<meta property="og:description" content="回顾前一篇文章，GPTQ给大模型带来了降本的可能，但存在性能不佳的问题，无法直接落地。经过迁移适配，我们将GPTQ的INT4 Kernel集成" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://towriting.com/blog/2023/06/19/gptq2/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-06-19T20:41:48+08:00" />
<meta property="article:modified_time" content="2023-06-19T20:41:48+08:00" />
<meta itemprop="name" content="大模型推理 - GPTQ 落地与优化">
<meta itemprop="description" content="回顾前一篇文章，GPTQ给大模型带来了降本的可能，但存在性能不佳的问题，无法直接落地。经过迁移适配，我们将GPTQ的INT4 Kernel集成"><meta itemprop="datePublished" content="2023-06-19T20:41:48+08:00" />
<meta itemprop="dateModified" content="2023-06-19T20:41:48+08:00" />
<meta itemprop="wordCount" content="5021">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="大模型推理 - GPTQ 落地与优化"/>
<meta name="twitter:description" content="回顾前一篇文章，GPTQ给大模型带来了降本的可能，但存在性能不佳的问题，无法直接落地。经过迁移适配，我们将GPTQ的INT4 Kernel集成"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

  
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-R5GWFRP16Y"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-R5GWFRP16Y');
</script>
  

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">towriting</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/blog/my-books">
        <li class="mobile-menu-item">Books</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">towriting</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/blog/my-books">Books</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">大模型推理 - GPTQ 落地与优化</h1>

      <div class="post-meta">
        <span class="post-time"> 2023-06-19 </span>
        <div class="post-category">
            <a href="/categories/%E5%BC%80%E5%8F%91/"> 开发 </a>
            </div>
          <span class="more-meta"> 5021 words </span>
          <span class="more-meta"> 11 mins read </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#一ft改造">一、FT改造</a>
          <ul>
            <li><a href="#权重矩阵">权重矩阵</a></li>
            <li><a href="#转换脚本">转换脚本</a></li>
            <li><a href="#模型加载">模型加载</a></li>
            <li><a href="#替换矩阵乘kernel">替换矩阵乘kernel</a></li>
          </ul>
        </li>
        <li><a href="#二kernel性能摸底">二、kernel性能摸底</a>
          <ul>
            <li><a href="#1-原始实现-old-cuda">1. 原始实现-old-cuda</a></li>
            <li><a href="#2-half2实现fastest-inference-4bit">2. half2实现fastest-inference-4bit</a></li>
          </ul>
        </li>
        <li><a href="#三kernel性能优化">三、kernel性能优化</a>
          <ul>
            <li><a href="#优化1-scale和zero读取优化">优化1： scale和zero读取优化</a></li>
            <li><a href="#优化2解决m增加耗时增加问题">优化2：解决m增加耗时增加问题</a></li>
            <li><a href="#优化3提高带宽利用率">优化3：提高带宽利用率</a></li>
            <li><a href="#优化4-解决m16后耗时线性增长的问题">优化4： 解决M&gt;16后，耗时线性增长的问题</a></li>
            <li><a href="#kernel优化的套路">kernel优化的套路</a></li>
          </ul>
        </li>
        <li><a href="#四未完的事">四、未完的事</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>回顾前一篇<a href="/blog/2023/04/26/gptq/">文章</a>，GPTQ给大模型带来了降本的可能，但存在性能不佳的问题，无法直接落地。经过迁移适配，我们将GPTQ的INT4 Kernel集成进<a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a>（简称FT），优化后可以在2卡A100运行175B的模型，对比fp16相同算力下性能提升近4倍。</p>
<p>看一下数据：</p>
<table>
<thead>
<tr>
<th>模型版本</th>
<th>模型大小</th>
<th>初始显存</th>
<th>峰值显存</th>
<th>显卡(A100)数</th>
<th>算力</th>
<th>每秒生成token数</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>329G</td>
<td>49G*8</td>
<td>无数据</td>
<td>8卡</td>
<td>99%</td>
<td>23</td>
</tr>
<tr>
<td>INT8</td>
<td>329G</td>
<td>49G*4</td>
<td>50G*4</td>
<td>4卡</td>
<td>99%</td>
<td>27</td>
</tr>
<tr>
<td>INT4-old-cuda</td>
<td>93G</td>
<td>52G*2</td>
<td>54G*2</td>
<td>2卡</td>
<td>99%</td>
<td>10</td>
</tr>
<tr>
<td>INT4-fastest-inference-4bit</td>
<td>93G</td>
<td>52G*2</td>
<td>54G*2</td>
<td>2卡</td>
<td>99%</td>
<td>12</td>
</tr>
<tr>
<td>INT4-优化版本</td>
<td>93G</td>
<td>52G*2</td>
<td>54G*2</td>
<td>2卡</td>
<td>99%</td>
<td>22</td>
</tr>
</tbody>
</table>
<p>测试基于FasterTransformer, 基础模型为bloom 175B, 模型版本:</p>
<ul>
<li>FP16 - 默认的推理方式，配置8卡Tensor并行，Linear算子调用的cublas的gemm</li>
<li>INT8 - Weight Only量化，算子基于cutlass::gemm::kernel::GemmFpAIntB</li>
<li><a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/old-cuda/quant_cuda_kernel.cu">INT4-old-cuda</a> GPTQ最初的开源实现，VecQuant4MatMulKernel是INT4的矩阵乘算子</li>
<li><a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/fastest-inference-4bit/quant_cuda_kernel.cu">INT4-fastest-inference-4bit</a> GPTQ-for-LLaMa项目对INT4算子做了优化，主要的改动是2个half变成一个half2</li>
<li>INT4-优化版本 我们基于fastest-inference-4bit版本做了一些优化</li>
</ul>
<h2 id="一ft改造">一、FT改造</h2>
<p>改造FT以支持GPTQ的INT4比较简单，首先了解下int4的权重文件。</p>
<h3 id="权重矩阵">权重矩阵</h3>
<p><img src="/images/posts/gptq/int4_weight.png" alt="int4权重"></p>
<p>INT4权重说明：</p>
<ol>
<li>一个Linear层的fp16权重，输出3个权重：int4的权重，scale系数，zero。</li>
<li>三个权重的类型和尺寸。假设原始fp16权重为[K, N], 则:</li>
</ol>
<ul>
<li>int4_weight为[K/8, N] (一列的8个int4合并一个int32)</li>
<li>scale为[K/128, N] (格式为fp16, 一列的128个权重共用一个scale, 128为量化时配置的groupsize)</li>
<li>zero为[K/128, N/8] (一行8个int4合并一个int32, 一列的128个权重共用一个int4的zero)</li>
</ul>
<h3 id="转换脚本">转换脚本</h3>
<p>修改<a href="https://github.com/NVIDIA/FasterTransformer/blob/main/examples/pytorch/gpt/utils/huggingface_bloom_convert.py">huggingface_bloom_convert.py</a>以支持转换int4权重，流程简单，关键代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 处理int4权重，转换名字，保存成numpy格式, model_file为gptq int4量化后保存的权重文件</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_process_quant_file</span><span class="p">(</span><span class="n">model_config</span><span class="p">,</span> <span class="n">model_file</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">tp_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">:</span> <span class="n">Path</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_file</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">param</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Preprocess</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_name</span> <span class="o">=</span> <span class="n">convert_parameter_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">param_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">((</span><span class="s2">&#34;qzeros&#34;</span><span class="p">,</span> <span class="s2">&#34;scales&#34;</span><span class="p">,</span> <span class="s2">&#34;qweight&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># param = safe_transpose(param) 注意不要转置，因为gqtq已转置</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">        <span class="n">param</span> <span class="o">=</span> <span class="n">handle_exceptions</span><span class="p">(</span><span class="n">model_config</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">convert_and_save_parameter</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">tp_size</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># qkv权重reorder</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">reorder_qkv_weight_or_bias</span><span class="p">(</span><span class="n">model_config</span><span class="p">:</span> <span class="n">PretrainedConfig</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                               <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                               <span class="n">param</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s1">&#39;query_key_value&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Nothing to do for the non-eligible parameters.</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">param</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">n_head</span>
</span></span><span class="line"><span class="cl">    <span class="n">head_dim</span> <span class="o">=</span> <span class="n">model_config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">model_config</span><span class="o">.</span><span class="n">n_head</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s2">&#34;qzeros&#34;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="o">//</span> <span class="mi">8</span> <span class="c1"># qzero 一行8个int4合并int32, 所以需要除8</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># (..., 3 x hidden) view as (..., num_heads, 3, head_dim)</span>
</span></span><span class="line"><span class="cl">    <span class="n">param</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># permute to (..., 3, num_heads, head_dim)</span>
</span></span><span class="line"><span class="cl">    <span class="n">param</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># final shape: weight=(hidden, 3, hidden) or bias=(3, hidden)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="s1">&#39;query_key_value.bias&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">param</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">param</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="模型加载">模型加载</h3>
<p>修改ParallelGptDecoderLayerWeight<T>::loadModel函数，以加载int4的权重文件，比较简单，不再赘述。</p>
<h3 id="替换矩阵乘kernel">替换矩阵乘kernel</h3>
<p>封装<a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/old-cuda/quant_cuda_kernel.cu">vecquant4matmul_cuda</a>算子成GptqGemmRunner类，然后替换掉</p>
<ul>
<li>self_attention.dense</li>
<li>self_attention.query_key_value</li>
<li>mlp.dense_4h_to_h</li>
<li>mlp.dense_h_to_4h等处的调用, 示例：</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// src/fastertransformer/layers/attention_layers/DecoderSelfAttentionLayer.cc
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span> <span class="n">T</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">DecoderSelfAttentionLayer</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;::</span><span class="n">forward</span><span class="p">(</span><span class="n">TensorMap</span><span class="o">*</span>                <span class="n">output_tensors</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="n">TensorMap</span><span class="o">*</span>                <span class="n">input_tensors</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                           <span class="k">const</span> <span class="n">AttentionWeight</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;*</span> <span class="n">attention_weights</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">int8_mode_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// int8 weight only gemm
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">int8_mode_</span> <span class="o">==</span> <span class="mi">10</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// gptq gemm
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="n">gptq_runner_</span><span class="o">-&gt;</span><span class="n">gemm</span><span class="p">(</span><span class="n">attention_weights</span><span class="o">-&gt;</span><span class="n">query_weight</span><span class="p">.</span><span class="n">qweight</span><span class="p">,</span> <span class="c1">// int4的权重 buf 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                              <span class="p">{</span><span class="n">d_model_</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">local_hidden_units_</span><span class="p">},</span> <span class="c1">// 权重的尺寸
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                               <span class="n">attention_weights</span><span class="o">-&gt;</span><span class="n">query_weight</span><span class="p">.</span><span class="n">qscales</span><span class="p">,</span>  <span class="c1">// 量化系数 buf
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                               <span class="n">attention_weights</span><span class="o">-&gt;</span><span class="n">query_weight</span><span class="p">.</span><span class="n">qzeros</span><span class="p">,</span>   <span class="c1">// 零点值 buf
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                               <span class="n">attention_input</span><span class="p">,</span> <span class="c1">// 输入的matrix buf 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                               <span class="p">{</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">d_model_</span><span class="p">},</span>  <span class="c1">// 输入的尺寸
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                               <span class="n">qkv_buf_</span><span class="p">,</span> <span class="c1">// 输出buf
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                               <span class="n">stream_</span><span class="p">,</span> <span class="n">cublas_wrapper_</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="p">{</span> <span class="c1">// fp16
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">cublas_wrapper_</span><span class="o">-&gt;</span><span class="n">Gemm</span><span class="p">(</span><span class="n">CUBLAS_OP_N</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">CUBLAS_OP_N</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="mi">3</span> <span class="o">*</span> <span class="n">local_hidden_units_</span><span class="p">,</span>  <span class="c1">// n
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">d_model_</span><span class="p">,</span>  <span class="c1">// k
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">attention_weights</span><span class="o">-&gt;</span><span class="n">query_weight</span><span class="p">.</span><span class="n">kernel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="mi">3</span> <span class="o">*</span> <span class="n">local_hidden_units_</span><span class="p">,</span>  <span class="c1">// n
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">attention_input</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">d_model_</span><span class="p">,</span>  <span class="c1">// k
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">qkv_buf_</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="mi">3</span> <span class="o">*</span> <span class="n">local_hidden_units_</span> <span class="cm">/* n */</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="二kernel性能摸底">二、kernel性能摸底</h2>
<p>调通流程不难，难点在于kernel算子性能，未优化的int4 kernel性能不及FP16，不符合预期，性能优化势在必行。</p>
<h3 id="1-原始实现-old-cuda">1. 原始实现-old-cuda</h3>
<p>参加代码<a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/old-cuda/quant_cuda_kernel.cu">VecQuant4MatMulKernel</a>:</p>
<ul>
<li>功能：矩阵乘，假设C=A * B, A=[M, K], B=[K, N], C=[M, N]</li>
<li>scalar_t为fp16, vec为A[M, K], mat为int4权重[K/8, N], mul为结果C[M, N], scales为[K/128, N], zeros为[K/128, N/8]</li>
</ul>
<p>计算过程：</p>
<ul>
<li>将mat切分为[BLOCKWIDTH, BLOCKWIDTH]的子矩阵，每个block处理[1, BLOCKWIDTH] * [BLOCKWIDTH, BLOCKWIDTH]的矩阵乘，子结果通过atomicAdd更新到mul中。</li>
<li>K/BLOCKWIDTH个block的结果相加得到mul[1, BLOCKWIDTH]</li>
<li>{K/BLOCKWIDTH, N/BLOCKWIDTH}个block处理[1, N]个结果</li>
<li>{K/BLOCKWIDTH, N/BLOCKWIDTH, M}个block得到[M, N]个结果</li>
<li>每个线程处理BLOCKWIDTH个乘加</li>
</ul>
<p>线程内的流程：</p>
<ul>
<li>BLOCKWIDTH个线程协作拷贝vec中BLOCKWIDTH个值到share memory中（变量blockvec）。</li>
<li>从mat中获取Int32的权重，通过scale和zero还原成8个fp16，与blockvec中的值进行乘加。</li>
<li>将乘加的结果通过atomicAdd保存到mul中。</li>
</ul>
<p>kernel算子性能测试：</p>
<ul>
<li>测试环境：A100单卡，权重为bloom 175B 2卡query_key_value的尺寸：14336*21504</li>
<li>矩阵乘算子耗时对比，时间单位（微秒）</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">M</th>
<th style="text-align:center">cublas FP16耗时</th>
<th style="text-align:center">int4算子-BLOCKWIDTH-256</th>
<th style="text-align:center">int4算子-BLOCKWIDTH-1024</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">372.794</td>
<td style="text-align:center">419.525</td>
<td style="text-align:center">271.103</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">373.715</td>
<td style="text-align:center">1308.533</td>
<td style="text-align:center">515.462</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">373.525</td>
<td style="text-align:center">2544.975</td>
<td style="text-align:center">958.13</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">378.458</td>
<td style="text-align:center">5040.562</td>
<td style="text-align:center">1905.114</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">384.483</td>
<td style="text-align:center">10024.748</td>
<td style="text-align:center">3815.136</td>
</tr>
<tr>
<td style="text-align:center">128</td>
<td style="text-align:center">395.75</td>
<td style="text-align:center">79716.668</td>
<td style="text-align:center">30248.278</td>
</tr>
<tr>
<td style="text-align:center">256</td>
<td style="text-align:center">704.908</td>
<td style="text-align:center">159450.097</td>
<td style="text-align:center">61029.231</td>
</tr>
<tr>
<td style="text-align:center">320</td>
<td style="text-align:center">967.704</td>
<td style="text-align:center">199256.5</td>
<td style="text-align:center">76277.884</td>
</tr>
</tbody>
</table>
<p>结论：默认的实现有一些问题：</p>
<ol>
<li>int4比fp16约快37%(m=1)，不符合预期，应该更快。因为带宽约降低75%，算力增加了约2倍，但带宽一般是瓶颈。</li>
<li>默认的BLOCKWIDTH配置的太小，计算访存比低，测试下来1024最优。</li>
<li>随着M的增加，耗时线性增加，原因是多个M未做还原复用。</li>
</ol>
<h3 id="2-half2实现fastest-inference-4bit">2. half2实现fastest-inference-4bit</h3>
<p>默认的kernel实现性能不理想，然后发现GPTQ-for-LLaMa项目里面针对int4进行了专项的<a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa/issues/227">优化</a>.</p>
<p>优化思想:</p>
<ul>
<li>向量化读取，将读取vec的过程改成half2的格式，加快读取的速度</li>
<li>乘加计算也通过half2的类型，减少算力占用</li>
</ul>
<p>经过测试fastest-inference-4bit约提升18%，随着M的增加，耗时线性增加依旧。</p>
<h2 id="三kernel性能优化">三、kernel性能优化</h2>
<p>因为性能依然不理想，所以基于fastest-inference-4bit进行优化，经过三轮优化，最终实现：</p>
<ol>
<li>耗时降低到cublas fp16耗时的31% (m=1)</li>
<li>耗时降低到fastest-inference-4bit的51% (m=1)</li>
<li>初步解决了m增加，耗时线性增加到问题, m=16时耗时降低到fastest-inference-4bit的10%</li>
</ol>
<p>看一下数据：</p>
<ul>
<li>测试环境：A100单卡，权重为bloom 175B 2卡query_key_value的尺寸：14336*21504</li>
<li>矩阵乘算子耗时对比，时间单位（微秒）</li>
<li>BLOCKWIDTH=1024</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">m</th>
<th style="text-align:center">cublas耗时</th>
<th style="text-align:center">int4-old-cuda-耗时</th>
<th style="text-align:center">int4-fastest-inference-4bit</th>
<th style="text-align:center">优化1</th>
<th style="text-align:center">优化2</th>
<th style="text-align:center">优化3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">372.794</td>
<td style="text-align:center">271.103</td>
<td style="text-align:center">228.472</td>
<td style="text-align:center">178.361</td>
<td style="text-align:center">147.065</td>
<td style="text-align:center">117.33</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">373.715</td>
<td style="text-align:center">515.462</td>
<td style="text-align:center">430.873</td>
<td style="text-align:center">330.344</td>
<td style="text-align:center">215.531</td>
<td style="text-align:center">140.514</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">373.525</td>
<td style="text-align:center">958.13</td>
<td style="text-align:center">807.954</td>
<td style="text-align:center">614.537</td>
<td style="text-align:center">298.237</td>
<td style="text-align:center">169.338</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">378.458</td>
<td style="text-align:center">1905.114</td>
<td style="text-align:center">1622.786</td>
<td style="text-align:center">1202.168</td>
<td style="text-align:center">386.775</td>
<td style="text-align:center">266.117</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">384.483</td>
<td style="text-align:center">3815.136</td>
<td style="text-align:center">3266.978</td>
<td style="text-align:center">2411.934</td>
<td style="text-align:center">762.359</td>
<td style="text-align:center">348.608</td>
</tr>
<tr>
<td style="text-align:center">128</td>
<td style="text-align:center">395.75</td>
<td style="text-align:center">30248.278</td>
<td style="text-align:center">26160.017</td>
<td style="text-align:center">19724.196</td>
<td style="text-align:center">6112.127</td>
<td style="text-align:center">2639.796</td>
</tr>
<tr>
<td style="text-align:center">256</td>
<td style="text-align:center">704.908</td>
<td style="text-align:center">61029.231</td>
<td style="text-align:center">52545.124</td>
<td style="text-align:center">39206.874</td>
<td style="text-align:center">12224.146</td>
<td style="text-align:center">5231.433</td>
</tr>
<tr>
<td style="text-align:center">320</td>
<td style="text-align:center">967.704</td>
<td style="text-align:center">76277.884</td>
<td style="text-align:center">65818.762</td>
<td style="text-align:center">48847.196</td>
<td style="text-align:center">18336.961</td>
<td style="text-align:center">6698.273</td>
</tr>
</tbody>
</table>
<p>讲一下3轮优化的思路：</p>
<h3 id="优化1-scale和zero读取优化">优化1： scale和zero读取优化</h3>
<p>优化思想：</p>
<ul>
<li>合并减少读取scale和zero的次数 因为每128个权重共享一个scale和zero, 所以每128个int4权重读取一次scale和zero, 并不需要每8个int4读一次。</li>
<li>加大每个线程处理的结果数，每个线程处理2个结果。</li>
</ul>
<p>优化效果：相比fastest-inference-4bit耗时降低22%。</p>
<h3 id="优化2解决m增加耗时增加问题">优化2：解决m增加耗时增加问题</h3>
<p>优化思想：</p>
<ul>
<li>共享权重还原，多个m共享一次权重的读取和还原</li>
<li>减少bank冲突，加大deq2的冗余，减少冲突，当m=1，配置TBANK=32。</li>
</ul>
<p>工程实现注意事项：</p>
<ol>
<li>因为share memory的限制，最多16行vec共享一次权重读取和还原。m大于16时，16的整数倍通过配置blockIdx.z在一次kernel launch中执行，余数单独调用一次kernel launch</li>
</ol>
<p>优化效果：相比fastest-inference-4bit耗时降低35%, 且m增加耗时增加问题较大改善。</p>
<!-- ```

template <int BATCH, int TWIDTH, int TBANK, typename scalar_t>
__global__ void VecQuant4MatMulKernel_perf2(
    const  half2* __restrict__ vec,
    const    int* __restrict__ mat,
           scalar_t* __restrict__ mul,
    const  scalar_t* __restrict__ scales,
    const    int* __restrict__ zeros,
    int vec_height,
    int height,
    int width,
    int zero_width
) {
  const int blockwidth2 = TWIDTH / 2;
  int h = TWIDTH / 8 * blockIdx.x;
  int w = TWIDTH * blockIdx.y + threadIdx.x;
  int w2 = w + blockwidth2;

  __shared__ half2 blockvec[BATCH][TWIDTH];
  #pragma unroll
  for (int b=0; b<BATCH; ++b) {
      blockvec[b][threadIdx.x] = vec[b * vec_height + blockIdx.x * blockwidth2 + threadIdx.x];
    }

  __shared__ half2 deq2[256][TBANK];
  int val = threadIdx.x / TBANK;
  int off = threadIdx.x % TBANK;
  for (; val < 256; val += blockwidth2 / TBANK) {
    deq2[val][off] = __halves2half2(
       __int2half_rn(val & 0xF), __int2half_rn(val >> 4)
    );
  }

  __syncthreads();

  int i = width * h + w;
  int g_h = h * 8;
  int k = 0;

  int z_w = w / 8; 
  int z_mod = (w % 8) * 4;

  int z_w2 = w2 / 8; 
  int z_mod2 = (w2 % 8) * 4;

  float res[BATCH] = {0};
  float res2[BATCH] = {0};
  half2 resh2;

  unsigned int tmp;
  unsigned int tmp2;

  int g = g_h / GROUPSIZE;
  const int group_count = TWIDTH / GROUPSIZE;
  #pragma unroll
  for (int gi=0; gi<group_count; ++gi) {
   float scale_f = scales[g * width + w];
    half2 scale = __float2half2_rn(scale_f);
    half2 zero = __float2half2_rn(-(scale_f * (((as_unsigned(zeros[g * zero_width + z_w]) >> z_mod) & 0xF) + 1)));

    float scale_f2 = scales[g * width + w2];
    half2 scale2 = __float2half2_rn(scale_f2);
    half2 zero2 = __float2half2_rn(-(scale_f2 * (((as_unsigned(zeros[g * zero_width + z_w2]) >> z_mod2) & 0xF) + 1)));

    const int gk = GROUPSIZE / 2 * (gi+1);
    while (k < gk) {

      tmp = as_unsigned(mat[i]);
      tmp2 = as_unsigned(mat[i + blockwidth2]);

        half2 m0 = __hfma2(deq2[(tmp >>  0) & 0xff][off], scale, zero);
        half2 m1 = __hfma2(deq2[(tmp >>  8) & 0xff][off], scale, zero);
        half2 m2 = __hfma2(deq2[(tmp >>  16) & 0xff][off], scale, zero);
        half2 m3 = __hfma2(deq2[(tmp >>  24) & 0xff][off], scale, zero);

        half2 m2_0 = __hfma2(deq2[(tmp2 >>  0) & 0xff][off], scale2, zero2);
        half2 m2_1 = __hfma2(deq2[(tmp2 >>  8) & 0xff][off], scale2, zero2);
        half2 m2_2 = __hfma2(deq2[(tmp2 >>  16) & 0xff][off], scale2, zero2);
        half2 m2_3 = __hfma2(deq2[(tmp2 >>  24) & 0xff][off], scale2, zero2);
          
          #pragma unroll
          for (int b=0; b<BATCH; ++b) {
              resh2 = {};
              
              half2 v0 = blockvec[b][k + 0];
              half2 v1 = blockvec[b][k + 1];
              half2 v2 = blockvec[b][k + 2];
              half2 v3 = blockvec[b][k + 3];

              resh2 = __hfma2(m0, v0, resh2);
              resh2 = __hfma2(m1, v1, resh2);
              resh2 = __hfma2(m2, v2, resh2);
              resh2 = __hfma2(m3, v3, resh2);
              res[b] += __half2float(resh2.x) + __half2float(resh2.y);

              resh2 = {};
              resh2 = __hfma2(m2_0, v0, resh2);
              resh2 = __hfma2(m2_1, v1, resh2);
              resh2 = __hfma2(m2_2, v2, resh2);
              resh2 = __hfma2(m2_3, v3, resh2);
              res2[b] += __half2float(resh2.x) + __half2float(resh2.y);
          }
          i += width;
          k += 4;
        }

    g = g + 1;    
  }

  for (int b=0; b<BATCH; ++b) {
    atomicAdd(&mul[b * width + w], res[b]);
    atomicAdd(&mul[b * width + w2], res2[b]);
  }
}
``` -->
<h3 id="优化3提高带宽利用率">优化3：提高带宽利用率</h3>
<p>完成优化2后，用Nsight Compute分析瓶颈在mat读取。</p>
<p>优化思想：</p>
<ul>
<li>合并读取，一个线程连续读取2个连续的int32。</li>
<li>向量化写入, 合并写入2个half为一个half2。</li>
</ul>
<p>最终优化效果：</p>
<ol>
<li>耗时降低到fastest-inference-4bit的51% (m=1)</li>
<li>初步解决了m增加，耗时线性增加到问题, m=16时耗时降低到fastest-inference-4bit的10%</li>
</ol>
<p>最终矩阵乘代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">// int4 矩阵乘
</span></span><span class="line"><span class="cl">// BATCH: 每BATCH行共享mat的还原,。m&lt;16时，BATCH=m; m&gt;16时BATCH=16, 然后blockIdx.z=16的倍数，余数单独调用一次kernel。
</span></span><span class="line"><span class="cl">// TWIDTH: 每个block处理[BATCH, TWIDTH] * [TWIDTH, TWIDTH], 需根据显卡实际测试最优值。
</span></span><span class="line"><span class="cl">// TBANK: 控制deq2的冗余，减少bank冲突，取值范围[1, 32]
</span></span><span class="line"><span class="cl">// scalar_t: vec的数据类型，fp16
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">template &lt;int BATCH, int TWIDTH, int TBANK, typename scalar_t&gt;
</span></span><span class="line"><span class="cl">__global__ void VecQuant4MatMulKernel_perf(
</span></span><span class="line"><span class="cl">    const  half2* __restrict__ vec,
</span></span><span class="line"><span class="cl">    const    int* __restrict__ mat,
</span></span><span class="line"><span class="cl">           half2* __restrict__ mul,
</span></span><span class="line"><span class="cl">    const  scalar_t* __restrict__ scales,
</span></span><span class="line"><span class="cl">    const    int* __restrict__ zeros,
</span></span><span class="line"><span class="cl">    int vec_height,
</span></span><span class="line"><span class="cl">    int height,
</span></span><span class="line"><span class="cl">    int width,
</span></span><span class="line"><span class="cl">    int zero_width
</span></span><span class="line"><span class="cl">) 
</span></span><span class="line"><span class="cl">{
</span></span><span class="line"><span class="cl">  const int blockwidth2 = TWIDTH / 2;
</span></span><span class="line"><span class="cl">  int h = TWIDTH / 8 * blockIdx.x;
</span></span><span class="line"><span class="cl">  int w = TWIDTH * blockIdx.y + 2 * threadIdx.x;
</span></span><span class="line"><span class="cl">  int w2 = w + 1;
</span></span><span class="line"><span class="cl">  vec = vec + BATCH * blockIdx.z * vec_height;
</span></span><span class="line"><span class="cl">  mul = mul + BATCH * blockIdx.z * width / 2;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  __shared__ half2 blockvec[BATCH][TWIDTH];
</span></span><span class="line"><span class="cl">  #pragma unroll
</span></span><span class="line"><span class="cl">  for (int b=0; b&lt;BATCH; ++b) {
</span></span><span class="line"><span class="cl">      blockvec[b][threadIdx.x] = vec[b * vec_height + blockIdx.x * blockwidth2 + threadIdx.x];
</span></span><span class="line"><span class="cl">    }
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  __shared__ half2 deq2[256][TBANK];
</span></span><span class="line"><span class="cl">  int val = threadIdx.x / TBANK;
</span></span><span class="line"><span class="cl">  int off = threadIdx.x % TBANK;
</span></span><span class="line"><span class="cl">  for (; val &lt; 256; val += blockwidth2 / TBANK) {
</span></span><span class="line"><span class="cl">    deq2[val][off] = __halves2half2(
</span></span><span class="line"><span class="cl">       __int2half_rn(val &amp; 0xF), __int2half_rn(val &gt;&gt; 4)
</span></span><span class="line"><span class="cl">    );
</span></span><span class="line"><span class="cl">  }
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  __syncthreads();
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  int i = width * h + w;
</span></span><span class="line"><span class="cl">  int g_h = h * 8;
</span></span><span class="line"><span class="cl">  int k = 0;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  int z_w = w / 8; 
</span></span><span class="line"><span class="cl">  int z_mod = (w % 8) * 4;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  // int z_w2 = w2 / 8; 
</span></span><span class="line"><span class="cl">  int z_mod2 = (w2 % 8) * 4;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  // float res[BATCH] = {0};
</span></span><span class="line"><span class="cl">  // float res2[BATCH] = {0};
</span></span><span class="line"><span class="cl">  half2 resh[BATCH] = { {} };
</span></span><span class="line"><span class="cl">  half2 resh2[BATCH] = { {} };
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  unsigned int tmp;
</span></span><span class="line"><span class="cl">  unsigned int tmp2;
</span></span><span class="line"><span class="cl">  int g = g_h / GROUPSIZE;
</span></span><span class="line"><span class="cl">  const int group_count = TWIDTH / GROUPSIZE;
</span></span><span class="line"><span class="cl">  // #pragma unroll
</span></span><span class="line"><span class="cl">  for (int gi=0; gi&lt;group_count; ++gi) {
</span></span><span class="line"><span class="cl">    float scale_f = scales[g * width + w];
</span></span><span class="line"><span class="cl">    half2 scale = __float2half2_rn(scale_f);
</span></span><span class="line"><span class="cl">    unsigned tmp_zero = as_unsigned(zeros[g * zero_width + z_w]);
</span></span><span class="line"><span class="cl">    half2 zero = __float2half2_rn(-(scale_f * (((tmp_zero &gt;&gt; z_mod) &amp; 0xF) + 1)));
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    float scale_f2 = scales[g * width + w2];
</span></span><span class="line"><span class="cl">    half2 scale2 = __float2half2_rn(scale_f2);
</span></span><span class="line"><span class="cl">    half2 zero2 = __float2half2_rn(-(scale_f2 * (((tmp_zero &gt;&gt; z_mod2) &amp; 0xF) + 1)));
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    const int gk = GROUPSIZE / 2 * (gi+1);
</span></span><span class="line"><span class="cl">    while (k &lt; gk) { // blockvec一次取2个值
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        tmp = as_unsigned(mat[i]);
</span></span><span class="line"><span class="cl">        tmp2 = as_unsigned(mat[i + 1]);
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        half2 m0 = __hfma2(deq2[(tmp &gt;&gt;  0) &amp; 0xff][off], scale, zero);
</span></span><span class="line"><span class="cl">        half2 m1 = __hfma2(deq2[(tmp &gt;&gt;  8) &amp; 0xff][off], scale, zero);
</span></span><span class="line"><span class="cl">        half2 m2 = __hfma2(deq2[(tmp &gt;&gt;  16) &amp; 0xff][off], scale, zero);
</span></span><span class="line"><span class="cl">        half2 m3 = __hfma2(deq2[(tmp &gt;&gt;  24) &amp; 0xff][off], scale, zero);
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        half2 m2_0 = __hfma2(deq2[(tmp2 &gt;&gt;  0) &amp; 0xff][off], scale2, zero2);
</span></span><span class="line"><span class="cl">        half2 m2_1 = __hfma2(deq2[(tmp2 &gt;&gt;  8) &amp; 0xff][off], scale2, zero2);
</span></span><span class="line"><span class="cl">        half2 m2_2 = __hfma2(deq2[(tmp2 &gt;&gt;  16) &amp; 0xff][off], scale2, zero2);
</span></span><span class="line"><span class="cl">        half2 m2_3 = __hfma2(deq2[(tmp2 &gt;&gt;  24) &amp; 0xff][off], scale2, zero2);
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          #pragma unroll
</span></span><span class="line"><span class="cl">          for (int b=0; b&lt;BATCH; ++b) {
</span></span><span class="line"><span class="cl">              
</span></span><span class="line"><span class="cl">              half2 v0 = blockvec[b][k + 0];
</span></span><span class="line"><span class="cl">              half2 v1 = blockvec[b][k + 1];
</span></span><span class="line"><span class="cl">              half2 v2 = blockvec[b][k + 2];
</span></span><span class="line"><span class="cl">              half2 v3 = blockvec[b][k + 3];
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">              resh[b] = __hfma2(m0, v0, resh[b]);
</span></span><span class="line"><span class="cl">              resh[b] = __hfma2(m1, v1, resh[b]);
</span></span><span class="line"><span class="cl">              resh[b] = __hfma2(m2, v2, resh[b]);
</span></span><span class="line"><span class="cl">              resh[b] = __hfma2(m3, v3, resh[b]);
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">              resh2[b] = __hfma2(m2_0, v0, resh2[b]);
</span></span><span class="line"><span class="cl">              resh2[b] = __hfma2(m2_1, v1, resh2[b]);
</span></span><span class="line"><span class="cl">              resh2[b] = __hfma2(m2_2, v2, resh2[b]);
</span></span><span class="line"><span class="cl">              resh2[b] = __hfma2(m2_3, v3, resh2[b]);
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          }
</span></span><span class="line"><span class="cl">          i += width;
</span></span><span class="line"><span class="cl">          k += 4;
</span></span><span class="line"><span class="cl">        }
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    g = g + 1;    
</span></span><span class="line"><span class="cl">  }
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  w = blockwidth2 * blockIdx.y + threadIdx.x;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  for (int b=0; b&lt;BATCH; ++b) {
</span></span><span class="line"><span class="cl">    float resa = __half2float(resh[b].x) + __half2float(resh[b].y);
</span></span><span class="line"><span class="cl">    float resb = __half2float(resh2[b].x) + __half2float(resh2[b].y);
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    half2 result = make_half2(resa, resb);
</span></span><span class="line"><span class="cl">    atomicAdd(&amp;mul[b * width / 2 + w], result);
</span></span><span class="line"><span class="cl">    // atomicAdd(&amp;mul[b * width + w2], res2[b]);
</span></span><span class="line"><span class="cl">  }
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="优化4-解决m16后耗时线性增长的问题">优化4： 解决M&gt;16后，耗时线性增长的问题</h3>
<p>经过3轮优化后，m&lt;16时，int4算子比cublas有明显优势，但当m&gt;16时，int4算子耗时增加明显。</p>
<p>在FastTransformer中，有2个场景会用到m&gt;16:</p>
<ol>
<li>生成context阶段，即由embedding计算kv cache的时候，m=batch*input_token_len，m很容易超过16，一次推理调用一次，会影响首字耗时</li>
<li>生成一个token阶段，m=batch, 一般很难超过16</li>
</ol>
<p>因为m&gt;16会影响首字耗时，所以采用下列策略优化：</p>
<ol>
<li>回退cublas，因为cublas对多m优化较好，所以实现了一个反量化的接口，对于m&gt;48的场景先反量化到fp16，然后再调用cublas。48为实测下，反量化+cublas耗时小于int4算子耗时的拐点。</li>
<li>反量化需要显存，一块卡复用一个buf即可。</li>
</ol>
<h3 id="kernel优化的套路">kernel优化的套路</h3>
<p>因为我也是第一次做kernel优化，所以记录下：</p>
<ol>
<li>规划好block和thread的职责，先实现一个简单的版本</li>
<li>核对kernel结果是否正确</li>
<li>调整grid和block的size, 对比测试找到一个最优的值</li>
<li>使用 Nsight Compute分析kernel的瓶颈, 需要关注的点：</li>
</ol>
<ul>
<li>Occupancy，理论Occupancy和实际Occupancy</li>
<li>Compute (SM) Throughput 和 Memory Throughput</li>
<li>memory workload 里面的bank conflict</li>
<li>Source Counters 可以定位warp挂起的原因</li>
</ul>
<ol start="5">
<li>解决Nsight Compute分析出的瓶颈</li>
<li>常见的优化方法：多级存储，向量化读写</li>
</ol>
<h2 id="四未完的事">四、未完的事</h2>
<ol>
<li>继续优化int4的kernel, 在m&gt;16后，应该有更优雅的实现方法</li>
<li>实现并验证双buf预取方法，增加带宽的利用率</li>
<li>一些疑点：为何INT2向量化读慢于2个int32</li>
</ol>

    </div>

    <div class="post-copyright">
  
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license" href="http://creativecommons.org/licenses/by/3.0/cn/">知识共享署名 3.0 中国大陆许可协议</a></span>
  </p>
</div>
<footer class="post-footer">
      
      <nav class="post-nav">
        <a class="prev" href="/blog/2023/09/02/continuous-batch/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">大模型推理 - Continuous batching和FasterTransformer结合</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/blog/2023/04/26/gptq/">
            <span class="next-text nav-default">大模型推理 - GPTQ 量化过程解析</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>



    

  </article>
        </div>
        

  

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  <a href="http://towriting.com/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2013 - 
    2023<span class="heart"><i class="iconfont icon-heart"></i></span><span>bugliu 2023</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.bootcdn.net/ajax/libs/slideout/1.0.1/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.bootcdn.net/ajax/libs/fancybox/3.1.20/jquery.fancybox.min.js"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>








</body>
</html>
