---
title: "分布式向量检索系统Vearch应用实战"
date: 2021-10-07T19:29:25+08:00
draft: true
---

## 一、向量检索的工程特点

图像AI领域，其检索（或识别）一般是基于特征。项目落地如何衡量效果呢？主要关注两类指标，一类是算法效果，包括准确率、召回率等，一类是**工程效果**，包括耗时、资源占用。

### 1. 单个特征Size较大
具体到我们公司的REID算法，为了达到一个好的算法效果，采用了较高的维度（512维），这带来了单个特征Size较大(2K)，并且REID特征的不稳定性导致无法像人脸特征一样合并相同的特征。

### 2. 海量的冷数据
从统计规律看，项目中平均每路摄像头每天10W左右的特征数量(形体抓拍)。一个千路的项目，则每天产生的特征的大小为：`1000 * 10W * 512 * sizeof(float) = 200G`，客户一般要求数据保存6个月的数据，算一下是36T的特征，这个数据量是**海量**的。虽然总数据很大，但客户一般仅仅会检索最近7天的数据，而半个月前的数据往往是一直沉寂不用的。

### 3. 检索任务：耗时敏感、读IO密集型、计算密集型
检索是我们产品的高频操作主流程，检索速度直接关乎用户的体验。千万数据的检索可以1秒完成才算合格的产品体验。

检索的主要原理是从海量特征中对比查找和目标特征最相近的TopN的特征，而海量的数据导致检索任务：
* IO密集型，因为海量的数据注定不会全部加载内存，必然存在特征的按需从硬盘中加载。
* 计算密集型，特征的两两比对，一般采用[余弦距离][1]或者[欧式距离][2]，其算法复杂度和特征维度成O(N)的关系。而特征的检索，又和特征的总量成O(N)的关系。

IO和计算的密集型和检索速度是矛盾的，随着数据量的加大，势必严重制约着检索的速度。另外除了特征比对以外，检索需要支持**标签过滤**，能按一些业务维度比如时间、摄像头进行过滤。

### 4. 特征存储：顺序写入、不会修改、低频删除、低频回查

特征写入的TPS, 按实测数据每路摄像头1.2TPS, 峰值约为4TPS，千路的均值和峰值分别为1200TPS和4000TPS。单个特征大小2K, 千路的均值和峰值写入吞吐量分别为20Mbps和64Mbps。特征前后之间没有任何顺序关系，所以我们通过常规的缓存+顺序写入的方式是很容易满足性能要求的。

公安业务特征主要来自实时摄像头，特征不会修改，但需要删除，回收空间。业务上需要回查特征满足检索等产品需求。

## 二、向量检索系统方案

从前面向量检索系统的工程特点可知，一个优秀的向量检索系统需要具备**特征存储管理**和**检索**两项能力，并能应对**海量数据**的挑战。目前市面上主要方案如下：

1. 暴力检索。部分小团队（包括我们在19年）会采用这种方案，其主要方式就是将特征和相关数据都写入一个（或多个）文件中，然后检索时加载到内存。此方案好在简单，坏在暴力，在总数据量在百万一下可以应付，但无法应对千万以上的数据，耗时会在数分钟，是完全不可接受的。

2. [最邻近搜索][3]。

3. 分布式最邻近搜索。

## 三、Vearch介绍

### 术语

### 架构

### 流程

## 四、Vearch落地

### 小Bug

### 改造

* 冷数据模式
* 预训练


**参考**:
* [蚂蚁金服 ZSearch 在向量检索上的探索](https://www.infoq.cn/article/rb1dzi4t69ivvqfvjco7)


[1]: https://zh.wikipedia.org/zh-hans/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7
[2]: https://zh.wikipedia.org/zh-hans/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB
[3]: https://zh.wikipedia.org/wiki/%E6%9C%80%E9%82%BB%E8%BF%91%E6%90%9C%E7%B4%A2
